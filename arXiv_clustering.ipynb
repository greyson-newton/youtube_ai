{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "arXiv_clustering.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPK0X2aZAfsUFdi8aTz836N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/greyson-newton/youtube_ai/blob/master/arXiv_clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKLKmP25RXeQ"
      },
      "source": [
        "# Greyson Newton - Scientific Paper Clustering\n",
        "\n",
        "\n",
        "---\n",
        "Content:\n",
        "\n",
        "\n",
        "1.   Load data\n",
        "2.   data-cleaning / feature engineering\n",
        "3.   NLP data preprocessing\n",
        "4.   Vectorization & dim. reduction with PCA\n",
        "5.   Clustering\n",
        "6.   t-SNE vs umap\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bB3QPCUsQMo-"
      },
      "source": [
        "# Preamble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5G6k3EnQQll",
        "outputId": "fcb06d97-8a09-41bd-b890-9fbe7293b4e7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1u2Lc8yQRde"
      },
      "source": [
        "import os\n",
        "os.chdir(\"drive/My Drive/projects-deep_learning/ArXiv-NLP\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzwIpUTCsFFi"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_ie2DX7sKWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8080ea7c-17ca-4de7-d62c-4953e495e5af"
      },
      "source": [
        "!python -m pip install 'fsspec>=0.3.3'\n",
        "!pip install umap-learn\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fsspec>=0.3.3 in /usr/local/lib/python3.7/dist-packages (2021.11.0)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from umap-learn) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.5.5)\n",
            "Requirement already satisfied: numba>=0.49 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (0.51.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from umap-learn) (1.4.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.49->umap-learn) (57.4.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from pynndescent>=0.5->umap-learn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->umap-learn) (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_Hct-w4sIgx"
      },
      "source": [
        "# data processing\n",
        "import logging\n",
        "logger = logging.getLogger(\"spacy\")\n",
        "logger.setLevel(logging.ERROR)\n",
        "\n",
        "import dask.bag as db\n",
        "import json\n",
        "import pandas as pd\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP4D75Yiwz8L"
      },
      "source": [
        "# I discoverd that it's possible to download models for the specific purpose to preprocess scientific texts\n",
        "# In the spacy docs I found a specific model for this : https://spacy.io/universe/project/scispacy\n",
        "#Downloading en_core_sci_lg model to preprocess abstracts\n",
        "from IPython.utils import io\n",
        "# with io.capture_output() as captured:\n",
        "#     !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz\n",
        "\n",
        "\n",
        "# NLP Processing and Vectorization\n",
        "#Import NLP librarys and the spacy package to preprocess the abstract text\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS #import commen list of stopword\n",
        "import en_core_sci_lg  # import downlaoded model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yp9gkPJXwuSo"
      },
      "source": [
        "# Clustering\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy.spatial.distance import cdist\n",
        "from sklearn import metrics\n",
        "\n",
        "from umap import UMAP\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D5ot6AQQeBI"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRbsDy_Fb_en",
        "outputId": "16955321-5ab3-49ec-ba8b-f8333d553764"
      },
      "source": [
        "docs = db.read_text('data/arxiv_data.json').map(json.loads)\n",
        "#Total number of documents: 1872765\n",
        "docs.count().compute()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1963596"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VLpQLg9dMqt",
        "outputId": "73225f30-7368-47c9-85e3-f5f5c9246909"
      },
      "source": [
        "# Looking at one document:\n",
        "docs.take(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'abstract': '  A fully differential calculation in perturbative quantum chromodynamics is\\npresented for the production of massive photon pairs at hadron colliders. All\\nnext-to-leading order perturbative contributions from quark-antiquark,\\ngluon-(anti)quark, and gluon-gluon subprocesses are included, as well as\\nall-orders resummation of initial-state gluon radiation valid at\\nnext-to-next-to-leading logarithmic accuracy. The region of phase space is\\nspecified in which the calculation is most reliable. Good agreement is\\ndemonstrated with data from the Fermilab Tevatron, and predictions are made for\\nmore detailed tests with CDF and DO data. Predictions are shown for\\ndistributions of diphoton pairs produced at the energy of the Large Hadron\\nCollider (LHC). Distributions of the diphoton pairs from the decay of a Higgs\\nboson are contrasted with those produced from QCD processes at the LHC, showing\\nthat enhanced sensitivity to the signal can be obtained with judicious\\nselection of events.\\n',\n",
              "  'authors': \"C. Bal\\\\'azs, E. L. Berger, P. M. Nadolsky, C.-P. Yuan\",\n",
              "  'authors_parsed': [['Bal√°zs', 'C.', ''],\n",
              "   ['Berger', 'E. L.', ''],\n",
              "   ['Nadolsky', 'P. M.', ''],\n",
              "   ['Yuan', 'C. -P.', '']],\n",
              "  'categories': 'hep-ph',\n",
              "  'comments': '37 pages, 15 figures; published version',\n",
              "  'doi': '10.1103/PhysRevD.76.013009',\n",
              "  'id': '0704.0001',\n",
              "  'journal-ref': 'Phys.Rev.D76:013009,2007',\n",
              "  'license': None,\n",
              "  'report-no': 'ANL-HEP-PR-07-12',\n",
              "  'submitter': 'Pavel Nadolsky',\n",
              "  'title': 'Calculation of prompt diphoton production cross sections at Tevatron and\\n  LHC energies',\n",
              "  'update_date': '2008-11-26',\n",
              "  'versions': [{'created': 'Mon, 2 Apr 2007 19:18:42 GMT', 'version': 'v1'},\n",
              "   {'created': 'Tue, 24 Jul 2007 20:10:27 GMT', 'version': 'v2'}]},)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dvrf5nXZV2WL"
      },
      "source": [
        "# Process Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2YRYIJqHS80"
      },
      "source": [
        "# The dataset is very huge. Not sure if the whole set can be used. I start prototyping with a subset of the data so it's easyer to handel:\n",
        "# This procedure was recommended in the ArXiv dataset itself\n",
        "\n",
        "get_latest_version = lambda x: x['versions'][-1]['created']\n",
        "\n",
        "\n",
        "# get only necessary fields of the metadata file\n",
        "trim = lambda x: {'id': x['id'],\n",
        "                  'authors': x['authors'],\n",
        "                  'title': x['title'],\n",
        "                  'doi': x['doi'],\n",
        "                  'category':x['categories'].split(' '),\n",
        "                  'abstract':x['abstract'],}\n",
        "# filter for papers published on or after 2019-01-01\n",
        "columns = ['id','category','abstract']\n",
        "docs_df = (docs.filter(lambda x: int(get_latest_version(x).split(' ')[3]) > 2018)\n",
        "           .map(trim).\n",
        "           compute())\n",
        "\n",
        "# convert to pandas\n",
        "docs_df = pd.DataFrame(docs_df)\n",
        "\n",
        "#save trimmed dataset for later use so we can skip the dataset trimming later:\n",
        "docs_df.to_csv(\"trimmed_arxiv_docs.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "P8oXwcCys9Yp",
        "outputId": "a6e284be-9f07-432d-d133-c2bef0534f7f"
      },
      "source": [
        "#Let's have a look at the first 5 rows:\n",
        "\n",
        "\n",
        "docs_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>authors</th>\n",
              "      <th>title</th>\n",
              "      <th>doi</th>\n",
              "      <th>category</th>\n",
              "      <th>abstract</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0704.0479</td>\n",
              "      <td>T.Geisser</td>\n",
              "      <td>The affine part of the Picard scheme</td>\n",
              "      <td>None</td>\n",
              "      <td>[math.AG, math.KT]</td>\n",
              "      <td>We describe the maximal torus and maximal un...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0704.1445</td>\n",
              "      <td>Yasha Gindikin and Vladimir A. Sablikov</td>\n",
              "      <td>Deformed Wigner crystal in a one-dimensional q...</td>\n",
              "      <td>10.1103/PhysRevB.76.045122</td>\n",
              "      <td>[cond-mat.str-el, cond-mat.mes-hall]</td>\n",
              "      <td>The spatial Fourier spectrum of the electron...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0705.0033</td>\n",
              "      <td>Nikos Frantzikinakis, Randall McCutcheon</td>\n",
              "      <td>Ergodic Theory: Recurrence</td>\n",
              "      <td>None</td>\n",
              "      <td>[math.DS]</td>\n",
              "      <td>We survey the impact of the Poincar\\'e recur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0705.0344</td>\n",
              "      <td>J. P. Pridham</td>\n",
              "      <td>Unifying derived deformation theories</td>\n",
              "      <td>None</td>\n",
              "      <td>[math.AG]</td>\n",
              "      <td>We develop a framework for derived deformati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0705.0825</td>\n",
              "      <td>Ram Gopal Vishwakarma (Zacatecas University)</td>\n",
              "      <td>Einstein's Theory of Gravity in the Presence o...</td>\n",
              "      <td>10.1007/s10509-009-0016-8</td>\n",
              "      <td>[gr-qc, astro-ph, hep-th]</td>\n",
              "      <td>The mysterious `dark energy' needed to expla...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id                                       authors  \\\n",
              "0  0704.0479                                     T.Geisser   \n",
              "1  0704.1445       Yasha Gindikin and Vladimir A. Sablikov   \n",
              "2  0705.0033      Nikos Frantzikinakis, Randall McCutcheon   \n",
              "3  0705.0344                                 J. P. Pridham   \n",
              "4  0705.0825  Ram Gopal Vishwakarma (Zacatecas University)   \n",
              "\n",
              "                                               title  \\\n",
              "0               The affine part of the Picard scheme   \n",
              "1  Deformed Wigner crystal in a one-dimensional q...   \n",
              "2                         Ergodic Theory: Recurrence   \n",
              "3              Unifying derived deformation theories   \n",
              "4  Einstein's Theory of Gravity in the Presence o...   \n",
              "\n",
              "                          doi                              category  \\\n",
              "0                        None                    [math.AG, math.KT]   \n",
              "1  10.1103/PhysRevB.76.045122  [cond-mat.str-el, cond-mat.mes-hall]   \n",
              "2                        None                             [math.DS]   \n",
              "3                        None                             [math.AG]   \n",
              "4   10.1007/s10509-009-0016-8             [gr-qc, astro-ph, hep-th]   \n",
              "\n",
              "                                            abstract  \n",
              "0    We describe the maximal torus and maximal un...  \n",
              "1    The spatial Fourier spectrum of the electron...  \n",
              "2    We survey the impact of the Poincar\\'e recur...  \n",
              "3    We develop a framework for derived deformati...  \n",
              "4    The mysterious `dark energy' needed to expla...  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM4-w5dGV4ZH"
      },
      "source": [
        "# NLProcess Data\n",
        "\n",
        "Saved trimmed and feature engineered data to '/data/clustered_processed_papers.csv'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPikDJ7QuAdC",
        "outputId": "181181b8-18e0-42fb-e64d-e3cb0678dc99"
      },
      "source": [
        "df = pd.read_csv(\"./trimmed_arxiv_docs.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqxTkdK2uCEV",
        "outputId": "4330b50e-fd08-4d73-e4c2-7c56da3e4400"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 526647 entries, 0 to 526646\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count   Dtype \n",
            "---  ------    --------------   ----- \n",
            " 0   id        526647 non-null  object\n",
            " 1   authors   526647 non-null  object\n",
            " 2   title     526647 non-null  object\n",
            " 3   doi       181827 non-null  object\n",
            " 4   category  526647 non-null  object\n",
            " 5   abstract  526647 non-null  object\n",
            "dtypes: object(6)\n",
            "memory usage: 24.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trsD1uVluCf8",
        "outputId": "ba45b1bc-796d-497d-d930-b8ec5c2a13a7"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(526647, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKkhI2a6uOpH",
        "outputId": "e05acd16-b687-4b02-db9a-f6172b5343e9"
      },
      "source": [
        "#Addint word counts of each abstract could be a usefull feature\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    pos_family = {\n",
        "\t    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "\t    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "\t    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "\t    'adj' :  ['JJ','JJR','JJS'],\n",
        "\t    'adv' : ['RB','RBR','RBS','WRB']\n",
        "    }\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "def feature_engineering(df):\n",
        "\tdf['char_count'] = docs_df['abstract'].apply(len)\n",
        "\tdf['word_count'] = docs_df['abstract'].apply(lambda x: len(x.split()))\n",
        "\tdf['word_density'] = df['char_count'] / (df['word_count']+1)\n",
        "\tdf['punctuation_count'] = docs_df['abstract'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "\tdf['title_word_count'] = docs_df['abstract'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "\tdf['upper_case_word_count'] = docs_df['abstract'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\t\n",
        "\tdf['noun_count'] = docs_df['abstract'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "\tdf['verb_count'] = docs_df['abstract'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "\tdf['adj_count'] = docs_df['abstract'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "\tdf['adv_count'] = docs_df['abstract'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "\tdf['pron_count'] = docs_df['abstract'].apply(lambda x: check_pos_tag(x, 'pron'))\n",
        "feature_engineering(df)  \n",
        "df['abstract'].describe(include='all')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count                                                526647\n",
              "unique                                               526544\n",
              "top         arXiv admin note: This submission has been w...\n",
              "freq                                                      3\n",
              "Name: abstract, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dzn24NFwvnli"
      },
      "source": [
        "# punctuations = string.punctuation #list of punctuation to remove from text\n",
        "# stopwords = list(STOP_WORDS)\n",
        "# # Parser\n",
        "# parser = en_core_sci_lg.load()\n",
        "# parser.max_length = 7000000 #Limit the size of the parser\n",
        "# def spacy_tokenizer(sentence):\n",
        "#     ''' Function to preprocess text of scientific papers \n",
        "#         (e.g Removing Stopword and puntuations)'''\n",
        "#     mytokens = parser(sentence)\n",
        "#     mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ] # transform to lowercase and then split the scentence\n",
        "#     mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ] #remove stopsword an punctuation\n",
        "#     mytokens = \" \".join([i for i in mytokens]) \n",
        "#     return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWeGl-pIv3CX",
        "outputId": "0fe51987-cf99-4b79-f7bc-17db043615ac"
      },
      "source": [
        "# tqdm.pandas()\n",
        "# df[\"processed_text\"] = df[\"abstract\"].progress_apply(spacy_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 526647/526647 [6:10:33<00:00, 23.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84XmfHjtV4ql"
      },
      "source": [
        "# Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw8Muw4GWRDO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f7ed6b-b15d-4f13-f603-8699cadfeeb6"
      },
      "source": [
        "# import trimmed and engineered data\n",
        "\n",
        "# df.to_csv('data/clustered_processed_papers.csv')\n",
        "# df.to_csv('data/clustered_processed_papers.csv')\n",
        "df = pd.read_csv('data/clustered_processed_papers.csv')\n",
        "df = df.iloc[: , 1:]\n",
        "def vectorize(text, maxx_features):\n",
        "    vectorizer = TfidfVectorizer(max_features=maxx_features)\n",
        "    X = vectorizer.fit_transform(text)\n",
        "    return X"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (1) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX4YQL5kwj7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc8c13b9-9b2f-4a7d-aa36-2da476150fd0"
      },
      "source": [
        "#vectorize each processed abstract\n",
        "text = df['processed_text'].values\n",
        "X = vectorize(text, 2 ** 12) #arbitrary max feature -_> Hyperpara. for optimisation (?)\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(526647, 4096)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQR4HZYrwqXM"
      },
      "source": [
        "# n_batches = 100\n",
        "# for X_batch in np.array_split(X.toarray(), n_batches):\n",
        "#     inc_pca.partial_fit(X_batch)\n",
        "# inc_pca = IncrementalPCA(n_components = 2)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "\n",
        "\n",
        "pca = IncrementalPCA(n_components=2, batch_size=100) #Keep 95% of the variance\n",
        "X_reduced= pca.fit_transform(X.toarray())\n",
        "X_reduced.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnxHsomwWS_H"
      },
      "source": [
        "# Clusterization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjPFCca2WYnr"
      },
      "source": [
        "r_seed = 24\n",
        "cluster_errors = []\n",
        "\n",
        "for i in range(1, 50):\n",
        "    n_clusters = i\n",
        "    pipe_pca_kmean = Pipeline([(\"cluster\", KMeans(n_clusters=n_clusters, random_state=r_seed, verbose=0, n_jobs=1))]\n",
        "    )\n",
        "\n",
        "    pipe_pca_kmean.fit(X_reduced)\n",
        "    pipe_pca_kmean.predict(X_reduced)\n",
        "    cluster_errors.append(pipe_pca_kmean.named_steps[\"cluster\"].inertia_) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R3KclKwxCln"
      },
      "source": [
        "plt.clf()\n",
        "plt.plot(cluster_errors, \"o-\")\n",
        "plt.xlabel(\"k_clusters\")\n",
        "plt.ylabel(\"sum sq distances from mean\")\n",
        "plt.savefig('results-clusterization/figs/k_cluster_error.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq9HLZZoxFtr"
      },
      "source": [
        "k = 20 # optimal k found in elbow plot\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "y_pred = kmeans.fit_predict(X_reduced)\n",
        "df['kmean_clusters'] = y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic_9X6SQWZgJ"
      },
      "source": [
        "# t-SNE vs. UMap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4sBmxzlcWdML"
      },
      "source": [
        "# UMAP Definition:\n",
        "umap_embeddings = UMAP(n_neighbors=100, min_dist=0.3, n_components=2)\n",
        "X_umap = umap_embeddings.fit_transform(X_reduced)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xvIM_aNxULG"
      },
      "source": [
        "tsne = TSNE(verbose=1, perplexity=100, random_state=42)\n",
        "X_embedded = tsne.fit_transform(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T65L6yP2xfeI"
      },
      "source": [
        "# sns settings\n",
        "sns.set(rc={'figure.figsize':(15,15)})\n",
        "\n",
        "# colors\n",
        "palette = sns.color_palette(\"bright\", 1)\n",
        "\n",
        "# plot\n",
        "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], palette=palette)\n",
        "plt.title('t-SNE without Labels')\n",
        "plt.savefig(\"results-clusterization/figs/t-sne_arxvid.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3yKdSOxxiRx"
      },
      "source": [
        "# sns settings\n",
        "sns.set(rc={'figure.figsize':(15,15)})\n",
        "\n",
        "# colors\n",
        "palette = sns.color_palette(\"bright\", 1)\n",
        "\n",
        "# plot\n",
        "sns.scatterplot(x=X_umap[:,0], y=X_umap[:,1], palette=palette)\n",
        "plt.title('umap without Labels')\n",
        "plt.savefig(\"results-clusterization/figs/umap_arxvid.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjEfz0BTxk2u"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# sns settings\n",
        "sns.set(rc={'figure.figsize':(15,15)})\n",
        "\n",
        "# colors\n",
        "palette = sns.hls_palette(20, l=.4, s=.9)\n",
        "\n",
        "# plot\n",
        "sns.scatterplot(x=X_embedded[:,0], y=X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\n",
        "plt.title('t-SNE with Kmeans Labels')\n",
        "plt.savefig(\"results-clusterization/figs/cluster_tsne.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}